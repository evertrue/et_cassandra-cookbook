#!/usr/bin/env bash

set -e

<% if node['et_cassandra']['cronitor']['backups_incremental']['enabled'] -%>
/usr/local/bin/cronitor_backups_incremental run
<% end -%>

source /etc/cassandra/snapshots.conf

let upload_retries=5

root_dir=`dirname "<%= node['et_cassandra']['config']['data_file_directories'].first %>"`
data_dirs="<%= node['et_cassandra']['config']['data_file_directories'].join(' ') %>"
work_dir="${root_dir}/backup_work_dir"
old_pwd="$PWD"

work_dir_fs_id=`stat -f -c %i ${work_dir}`

# Ensure that $work_dir and $data_dirs are on the same filesystem
for data_dir in $data_dirs; do
  if [[ $(stat -f -c %i ${data_dir}) != ${work_dir_fs_id} ]]; then
    echo "ERROR: Backup work_dir ${work_dir} is not on the same filesystem with data_dir (${data_dir})"
    exit 99
  fi
done

mkdir -p "${work_dir}"

backup_date=`date +%Y-%m-%dT%H%M%S`

function contained_by {
  local i

  for i in "${@:2}"; do [[ "${i}" == "${1}" ]] && return 0; done
  return 1
}

function archive_and_upload {
  local keyspace="${1}"
  local dir_index="${2}"
  local cur_backup_date="${3}"
  local tarball="${4}"

  local backup_holding_dir="${work_dir}/${cur_backup_date}/${dir_index}"

  cd "${backup_holding_dir}"

  if [ -e "$tarball" ]; then
    echo "$tarball already exists! Bailing instead of overwriting it."
    exit 1
  fi

  # It's important to remove files in the backups directory as atomically
  # as possible because this directory is going to be updated as we read it
  tar -czf "$tarball" "${keyspace}"

  trap "cd \"$old_pwd\"; exit" INT TERM EXIT

  if [ -n "$AWS_ACCESS_KEY_ID" ]; then
    local access_key_flags="--access_key $AWS_ACCESS_KEY_ID --secret_key $AWS_SECRET_ACCESS_KEY"
  fi

  set +e
  let try=0
  until [ $try -ge $upload_retries ]; do
    s3cmd put \
      --quiet \
      ${access_key_flags} \
      --server-side-encryption \
      --region $REGION \
      "$tarball" \
      "s3://$BUCKET/cassandra/<%= node.chef_environment %>/<%= node['fqdn'] %>/incrementals/$cur_backup_date/$keyspace-$dir_index.tar.gz" \
      && break
    let try=$[$try+1]
    sleep <%= node['et_cassandra']['incrementals']['upload_retry_delay'] %>
  done
  set -e

  if [ $try -ge $upload_retries ]; then
    echo "Could not upload ${tarball} in ${upload_retries} tries."
    return 1
  fi

  rm -f "$tarball"
  rm -rf "${backup_holding_dir}/${keyspace}"

  trap - INT TERM EXIT
}

# Re-upload any existing backups
for old_backup in `find ${work_dir} -mindepth 1 -maxdepth 1 -type d -printf '%P '`; do
  for dir_index in `find ${work_dir}/${old_backup} -mindepth 1 -maxdepth 1 -type d -printf '%P '`; do
    backup_holding_dir="${work_dir}/${old_backup}/${dir_index}"
    for keyspace in `find ${work_dir}/${old_backup}/${dir_index} -mindepth 1 -maxdepth 1 -type d -printf '%P '`; do
      echo "Re-archiving old backup: ${old_backup}/${dir_index}/${keyspace}"

      tarball="${work_dir}/$keyspace-$dir_index.tar.gz"

      if [ -e "$tarball" ]; then
        echo "$tarball already exists! Bailing instead of overwriting it."
        exit 1
      fi

      retval=0

      archive_and_upload "${keyspace}" "${dir_index}" "${old_backup}" "${tarball}" || retval=$?

      if [ $retval -gt 0 ]; then
        echo "archive_and_upload failed with code ${retval}. Skipping keyspace ${keyspace} in data_dir ${data_dir}."
        continue
      fi
    done
    rmdir "${backup_holding_dir}"
  done
done

nodetool -h localhost flush

# Start a new backup...
let dir_index=1

for data_dir in $data_dirs; do
  for keyspace in `find $data_dir -maxdepth 1 -mindepth 1 -type d -exec basename {} \;`; do
    if contained_by "${keyspace}" "${SKIP_KEYSPACES[@]}"; then continue; fi

    backup_holding_dir="${work_dir}/${backup_date}/${dir_index}"
    tarball="${work_dir}/$keyspace-$dir_index.tar.gz"

    if [ -e "$tarball" ]; then
      echo "$tarball already exists! Bailing instead of overwriting it."
      exit 1
    fi

    for table in `find ${data_dir}/${keyspace} -maxdepth 1 -mindepth 1 -type d -exec basename {} \;`; do
      mkdir -p "${backup_holding_dir}/${keyspace}/${table}"
      if [ -d "${data_dir}/${keyspace}/${table}/backups" ]; then
        mv "${data_dir}/${keyspace}/${table}/backups" "${backup_holding_dir}/${keyspace}/${table}/"
      fi
    done

    archive_and_upload "${keyspace}" "${dir_index}" "${backup_date}" "${tarball}"
  done # keyspaces loop
  rmdir "${work_dir}/${backup_date}/${dir_index}"
  let dir_index++
done

rmdir "${work_dir}/${backup_date}"

cd "$old_pwd"

<% if node['et_cassandra']['cronitor']['backups_incremental']['enabled'] -%>
/usr/local/bin/cronitor_backups_incremental complete
<% end -%>
