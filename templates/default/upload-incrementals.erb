#!/usr/bin/env bash

set -e

<% if node['et_cassandra']['cronitor']['backups_incremental']['enabled'] -%>
/usr/local/bin/cronitor_backups_incremental run
<% end -%>

source /etc/cassandra/snapshots.conf

let upload_retries=<%= node['et_cassandra']['incrementals']['upload_retries'] %>

root_dir=`dirname "<%= node['et_cassandra']['config']['data_file_directories'].first %>"`
data_dirs="<%= node['et_cassandra']['config']['data_file_directories'].join(' ') %>"
old_pwd="$PWD"

root_dir_fs_id=`stat -f -c %i ${root_dir}`

# Ensure that $root_dir and $data_dirs are on the same filesystem
for data_dir in $data_dirs; do
  if [[ $(stat -f -c %i ${data_dir}) != ${root_dir_fs_id} ]]; then
    echo "ERROR: Backup root_dir ${root_dir} is not on the same filesystem with data_dir (${data_dir})"
    exit 99
  fi
done

work_dir="${root_dir}/backup_work_dir/incrementals"
mkdir -p "${work_dir}"

backup_date=`date +%Y-%m-%dT%H%M%S`

function contained_by {
  local i

  for i in "${@:2}"; do [[ "${i}" == "${1}" ]] && return 0; done
  return 1
}

function archive_and_upload {
  local keyspace="${1}"
  local dir_index="${2}"
  local cur_backup_date="${3}"
  local tarball="${4}"

  local backup_holding_dir="${work_dir}/${cur_backup_date}/${dir_index}"

  cd "${backup_holding_dir}"

  if [ -e "$tarball" ]; then
    echo "$tarball already exists! Bailing instead of overwriting it."
    exit 1
  fi

  # It's important to remove files in the backups directory as atomically
  # as possible because this directory is going to be updated as we read it
  tar -czf "$tarball" "${keyspace}"

  trap "cd \"$old_pwd\"; exit" INT TERM EXIT

  set +e

  if [ -n "$AWS_ACCESS_KEY_ID" ]; then
    local access_key_flags="--access_key $AWS_ACCESS_KEY_ID --secret_key $AWS_SECRET_ACCESS_KEY"
  fi

  let try=0
  until [ $try -ge $upload_retries ]; do
    set -e
    s3cmd put \
      --quiet \
      ${access_key_flags} \
      --server-side-encryption \
      --region $REGION \
      "$tarball" \
      "s3://$BUCKET/cassandra/<%= node.chef_environment %>/<%= node['fqdn'] %>/incrementals/$cur_backup_date/$keyspace-$dir_index.tar.gz" \
      && break
    echo "S3 Upload failed. Retrying ($try/$upload_retries)"
    let try=$[$try+1]
    sleep <%= node['et_cassandra']['incrementals']['upload_retry_delay'] %>
    set +e
  done

  set +e

  if [ $try -ge $upload_retries ]; then
    echo "Could not upload ${tarball} in ${upload_retries} tries."
    return 1
  fi

  set -e

  rm -f "$tarball"
  rm -rf "${backup_holding_dir}/${keyspace}"

  trap - INT TERM EXIT
}

function old_backups {
  find ${work_dir} -mindepth 1 -maxdepth 1 -type d -printf '%P '
}

function dir_indices {
  local subdir="${1}"

  find ${work_dir}/${subdir} -mindepth 1 -maxdepth 1 -type d -printf '%P '
}

function old_backup_keyspaces {
  local subdir="${1}"
  local index="${2}"

  find ${work_dir}/${subdir}/${index} -mindepth 1 -maxdepth 1 -type d -printf '%P '
}

# Re-upload any existing backups
for old_backup in $(old_backups); do
  for dir_index in $(dir_indices "$old_backup"); do
    for keyspace in $(old_backup_keyspaces "$old_backup" $dir_index); do
      echo "Re-archiving old backup: ${old_backup}/${dir_index}/${keyspace}"

      tarball="${TARBALL_DIR}/$keyspace-$dir_index.tar.gz"

      if [ -e "$tarball" ]; then
        echo "$tarball already exists! Bailing instead of overwriting it."
        exit 1
      fi

      retval=0

      archive_and_upload "${keyspace}" "${dir_index}" "${old_backup}" "${tarball}" || retval=$?

      if [ $retval -gt 0 ]; then
        echo "archive_and_upload failed with code ${retval}. Skipping keyspace ${keyspace} in data_dir ${data_dir}."
        continue
      fi
    done

    if [ -d "${work_dir}/${old_backup}/${dir_index}" ]; then
      rmdir "${work_dir}/${old_backup}/${dir_index}"
    fi
  done

  if [ -d "${work_dir}/${old_backup}" ]; then
    rmdir "${work_dir}/${old_backup}"
  fi
done

nodetool -h localhost flush

function keyspaces {
  local subdir="${1}"

  find $subdir -maxdepth 1 -mindepth 1 -type d -exec basename {} \;
}

# Start a new backup...
let dir_index=1

for data_dir in $data_dirs; do
  for keyspace in $(keyspaces "$data_dir"); do
    if contained_by "${keyspace}" "${SKIP_KEYSPACES[@]}"; then continue; fi

    backup_holding_dir="${work_dir}/${backup_date}/${dir_index}"
    tarball="${work_dir}/$keyspace-$dir_index.tar.gz"

    if [ -e "$tarball" ]; then
      echo "$tarball already exists! Bailing instead of overwriting it."
      exit 1
    fi

    for table in $(keyspaces "${data_dir}/${keyspace}"); do
      mkdir -p "${backup_holding_dir}/${keyspace}/${table}"
      if [ -d "${data_dir}/${keyspace}/${table}/backups" ]; then
        mv "${data_dir}/${keyspace}/${table}/backups" "${backup_holding_dir}/${keyspace}/${table}/"
      fi
    done

    set +e
    retval=0

    archive_and_upload "${keyspace}" "${dir_index}" "${backup_date}" "${tarball}" || retval=$?

    if [ $retval -gt 0 ]; then
      echo "archive_and_upload failed with code ${retval}. Skipping keyspace ${keyspace} in data_dir ${data_dir}."
      continue
    fi
    set -e
  done # keyspaces loop

  if [ -d "${work_dir}/${backup_date}/${dir_index}" ]; then
    rmdir "${work_dir}/${backup_date}/${dir_index}"
  fi

  let dir_index++
done

if [ -d "${work_dir}/${backup_date}" ]; then
  rmdir "${work_dir}/${backup_date}"
fi

cd "$old_pwd"

<% if node['et_cassandra']['cronitor']['backups_incremental']['enabled'] -%>
/usr/local/bin/cronitor_backups_incremental complete
<% end -%>
