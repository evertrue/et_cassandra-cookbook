#!/usr/bin/env bash

set -e

<% if node['et_cassandra']['cronitor']['backups_incremental']['enabled'] -%>
/usr/local/bin/cronitor_backups_incremental run
<% end -%>

source /etc/cassandra/snapshots.conf

root_dir=`dirname "<%= node['et_cassandra']['config']['data_file_directories'].first %>"`
data_dirs="<%= node['et_cassandra']['config']['data_file_directories'].join(' ') %>"
work_dir="${root_dir}/backup_work_dir"
old_pwd="$PWD"

root_dir_fs_id=`/usr/bin/stat -f -c %i ${root_dir}`

# Ensure that $work_dir and $data_dirs are on the same filesystem
for data_dir in $data_dirs; do
  if [[ $(stat -f -c %i ${data_dir}) != ${root_dir_fs_id} ]]; then
    echo "ERROR: Backup work_dir ${work_dir} is not on the same filesystem with data_dir (${data_dir})"
    exit 99
  fi
done

mkdir -p "${work_dir}"

backup_date=`date +%Y-%m-%dT%H%M%S`

function contained_by {
  local i

  for i in "${@:2}"; do [[ "${i}" == "${1}" ]] && return 0; done
  return 1
}

function archive_and_upload {
  local keyspace="${1}"
  local dir_index="${2}"
  local cur_backup_date="${3}"

  local backup_holding_dir="${work_dir}/${cur_backup_date}/${dir_index}"

  tarball="${work_dir}/$keyspace-$dir_index.tar.gz"

  cd "${backup_holding_dir}"

  if [ -e "$tarball" ]; then
    echo "$tarball already exists! Bailing instead of overwriting it."
    exit 1
  fi

  # It's important to remove files in the backups directory as atomically
  # as possible because this directory is going to be updated as we read it
  tar --remove-files -czf "$tarball" "${keyspace}"

  trap "cd \"$old_pwd\"; exit" INT TERM EXIT

  /usr/local/bin/s3cmd put \
    --quiet \
    --access_key $AWS_ACCESS_KEY_ID \
    --secret_key $AWS_SECRET_ACCESS_KEY \
    --server-side-encryption \
    --region $REGION \
    "$tarball" \
    "s3://$BUCKET/cassandra/<%= node.chef_environment %>/<%= node['fqdn'] %>/incrementals/$cur_backup_date/$keyspace-$dir_index.tar.gz"

  rm -f "$tarball"
  rm -rf "${backup_holding_dir}/${keyspace}"

  trap - INT TERM EXIT
}

# Re-upload any existing backups
for old_backup in `find ${work_dir} -mindepth 1 -maxdepth 1 -type d -printf '%P '`; do
  for dir_index in `find ${work_dir}/${old_backup} -mindepth 1 -maxdepth 1 -type d -printf '%P '`; do
    backup_holding_dir="${work_dir}/${old_backup}/${dir_index}"
    for keyspace in `find ${work_dir}/${old_backup}/${dir_index} -mindepth 1 -maxdepth 1 -type d -printf '%P '`; do
      echo "Re-archiving old backup: ${old_backup}/${dir_index}/${keyspace}"
      archive_and_upload "${keyspace}" "${dir_index}" "${old_backup}"
    done
    rmdir "${backup_holding_dir}"
  done
done


# Start a new backup...
let dir_index=1

for data_dir in $data_dirs; do
  for keyspace in `find $data_dir -maxdepth 1 -mindepth 1 -type d -exec basename {} \;`; do
    if contained_by "${keyspace}" "${SKIP_KEYSPACES[@]}"; then continue; fi

    backup_holding_dir="${work_dir}/${backup_date}/${dir_index}"

    for table in `find ${data_dir}/${keyspace} -maxdepth 1 -mindepth 1 -type d -exec basename {} \;`; do
      mkdir -p "${backup_holding_dir}/${keyspace}/${table}"
      if [ -d "${data_dir}/${keyspace}/${table}/backups" ]; then
        mv "${data_dir}/${keyspace}/${table}/backups" "${backup_holding_dir}/${keyspace}/${table}/"
      fi
    done

    archive_and_upload "${keyspace}" "${dir_index}" "${backup_date}"
  done # keyspaces loop
  rmdir "${work_dir}/${backup_date}/${dir_index}"
  let dir_index++
done

rmdir "${work_dir}/${backup_date}"

cd "$old_pwd"

<% if node['et_cassandra']['cronitor']['backups_incremental']['enabled'] -%>
/usr/local/bin/cronitor_backups_incremental complete
<% end -%>
